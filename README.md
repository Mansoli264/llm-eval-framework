# ğŸ§  LLM Evaluation Framework: Ask Docs, Measure Answers

This project is an **LLM Evaluation Framework** designed to test how well large language models (LLMs) answer questions based on documents â€” like technical manuals, research papers, or product guides.

Imagine uploading a PDF, asking questions about it, and then seeing how accurate and helpful the LLMâ€™s answers are. Thatâ€™s exactly what this app does!

---

## âœ¨ Key Features

âœ… **Ask Questions from Docs**  
Upload structured or unstructured content, ask questions, and get smart answers powered by LLMs.

âœ… **Semantic Similarity Evaluation**  
We don't just stop at getting answers â€” we **score them** using **semantic similarity**. That means: does the model really understand the context?

âœ… **Modular Design**  
- **Backend**: Built with FastAPI (clean REST APIs)  
- **Frontend**: Built with Streamlit (simple dashboard)  
- **Evaluation Engine**: Custom logic to evaluate model output  
- **Dockerized**: Easy to run anywhere (local or cloud)

---

## ğŸ› ï¸ Tech Stack

| Layer         | Tools Used                            |
|---------------|----------------------------------------|
| Backend       | Python, FastAPI, Uvicorn               |
| Frontend      | Python, Streamlit                      |
| Evaluation    | SentenceTransformers, Semantic Scoring |
| Data Handling | JSON, RAG-style input chunks           |
| Deployment    | Docker, Docker Compose                 |

---

## ğŸš€ How It Works (In Simple Steps)

1. Upload a document or data source (`prompts.json`)  
2. The backend breaks it into chunks and prepares questions  
3. An LLM (e.g., GPT-4) gives answers  
4. We evaluate the answers based on similarity with ground truth  
5. The dashboard shows scores and insights

---

## ğŸ“ Project Structure

llm-eval-framework/
â”‚
â”œâ”€â”€ backend/ â†’ FastAPI backend
â”‚ â”œâ”€â”€ app/ â†’ Core logic (evaluation, API)
â”‚ â””â”€â”€ requirements.txt â†’ Python deps for backend
â”‚
â”œâ”€â”€ frontend/ â†’ Streamlit dashboard
â”‚ â””â”€â”€ requirements.txt â†’ Python deps for frontend
â”‚
â”œâ”€â”€ metrics/ â†’ Custom scorers (e.g., semantic similarity)
â”œâ”€â”€ data/ â†’ Sample inputs (like prompts.json)
â”œâ”€â”€ docker-compose.yml â†’ Combined app orchestration
â”œâ”€â”€ README.md â†’ This file ğŸ˜„
â””â”€â”€ .gitignore â†’ Clean setup

yaml
Copy
Edit

---

## ğŸ§ª Evaluation Metrics Used

- **Cosine Similarity** between embeddings (thanks to `SentenceTransformers`)
- **BLEU / ROUGE** (optional)
- Ground truth vs model answer comparisons

---

## ğŸ§° Running the Project (Local with Docker)

Make sure Docker is installed, then:

```bash
git clone https://github.com/Mansoli264/llm-eval-framework.git
cd llm-eval-framework
docker-compose up --build
This will spin up:

FastAPI backend at http://localhost:8000

Streamlit frontend at http://localhost:8501

ğŸ§  Why This Project Matters
Whether you're building an AI assistant, chatbot, or RAG pipeline â€” evaluating model output is ğŸ”‘. This project gives you a plug-and-play evaluation engine you can improve and reuse anywhere.
Perfect for:
AI research
LLM fine-tuning
Real-world QA systems
Interviews or demo portfolios
